{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "from abc import abstractmethod\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorpack import ModelDesc\n",
    "from tensorpack.dataflow import AugmentImageComponent, BatchData, MultiThreadMapData, PrefetchDataZMQ, dataset, imgaug\n",
    "from tensorpack.input_source import QueueInput, StagingInput\n",
    "from tensorpack.models import regularize_cost\n",
    "from tensorpack.predict import FeedfreePredictor, PredictConfig\n",
    "from tensorpack.tfutils.summary import add_moving_summary\n",
    "from tensorpack.utils import logger\n",
    "from tensorpack.utils.stats import RatioCounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A minimum example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorpack import DataFlow, DataFromGenerator\n",
    "from tensorpack.dataflow.parallel import PlasmaGetData, PlasmaPutData  # noqa\n",
    "def my_data_loader():\n",
    "  # load data from somewhere with Python, and yield them\n",
    "  for k in range(100):\n",
    "    yield [my_array, my_label]\n",
    "\n",
    "df = DataFromGenerator(my_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[0.56543431 0.46338831]\n",
      " [0.28324179 0.32366504]] 7\n",
      "\n",
      "[[0.4780968  0.03412478]\n",
      " [0.96600137 0.96960545]] 9\n",
      "\n",
      "[[0.35308466 0.12336586]\n",
      " [0.4294453  0.78506172]] 0\n"
     ]
    }
   ],
   "source": [
    "# a DataFlow you implement to produce [tensor1, tensor2, ..] lists from whatever sources:\n",
    "class MyDataFlow(DataFlow):\n",
    "  def __iter__(self):\n",
    "    # load data from somewhere with Python, and yield them\n",
    "#     ds = PrintData(ds, num=2, max_list=2)\n",
    " \n",
    "    for k in range(10):\n",
    "      digit = np.random.rand(2, 2)\n",
    "      label = np.random.randint(10)\n",
    "      yield [digit, label]\n",
    "      \n",
    "df = MyDataFlow()\n",
    "df = BatchData(df, 3)\n",
    "# df = PlasmaGetData(df)\n",
    "df.reset_state()\n",
    "for datapoint in df:\n",
    "    \n",
    "    print(\"\")\n",
    "    print(datapoint[0][0], datapoint[1][0])\n",
    "# print(df[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A higher level demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a DataFlow you implement to produce [tensor1, tensor2, ..] lists from whatever sources:\n",
    "class MyDataFlow(DataFlow):\n",
    "  def __iter__(self):\n",
    "    # load data from somewhere with Python, and yield them\n",
    "#     ds = PrintData(ds, num=2, max_list=2)\n",
    "    for k in range(10):\n",
    "      digit = np.random.rand(2, 2)\n",
    "      label = np.random.randint(10)\n",
    "      yield [digit, label]\n",
    "      \n",
    "df = MyDataFlow()\n",
    "df = BatchData(df, 3)\n",
    "# df = PlasmaGetData(df)\n",
    "df.reset_state()\n",
    "for datapoint in df:\n",
    "    \n",
    "    print(\"\")\n",
    "    print(datapoint[0][0], datapoint[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_orig(dir, name, augs, batch):\n",
    "    ds = dataset.ILSVRC12(dir, name, shuffle=True)\n",
    "    ds = AugmentImageComponent(ds, augs)\n",
    "\n",
    "    ds = BatchData(ds, batch)\n",
    "    # ds = PlasmaPutData(ds)\n",
    "    ds = PrefetchDataZMQ(ds, 50, hwm=80)\n",
    "    return ds\n",
    "\n",
    "def test_lmdb_train(db, augs, batch):\n",
    "    ds = LMDBData(db, shuffle=False)\n",
    "    ds = LocallyShuffleData(ds, 50000)\n",
    "    ds = PrefetchData(ds, 5000, 1)\n",
    "    return ds\n",
    "\n",
    "    ds = LMDBDataPoint(ds)\n",
    "\n",
    "    def f(x):\n",
    "        return cv2.imdecode(x, cv2.IMREAD_COLOR)\n",
    "    ds = MapDataComponent(ds, f, 0)\n",
    "    ds = AugmentImageComponent(ds, augs)\n",
    "\n",
    "    ds = BatchData(ds, batch, use_list=True)\n",
    "    # ds = PlasmaPutData(ds)\n",
    "    ds = PrefetchDataZMQ(ds, 40, hwm=80)\n",
    "    # ds = PlasmaGetData(ds)\n",
    "    return ds\n",
    "\n",
    "def test_inference(dir, name, augs, batch=128):\n",
    "    ds = dataset.ILSVRC12Files(dir, name, shuffle=False, dir_structure='train')\n",
    "\n",
    "    aug = imgaug.AugmentorList(augs)\n",
    "\n",
    "    def mapf(dp):\n",
    "        fname, cls = dp\n",
    "        im = cv2.imread(fname, cv2.IMREAD_COLOR)\n",
    "        im = aug.augment(im)\n",
    "        return im, cls\n",
    "    ds = ThreadedMapData(ds, 30, mapf, buffer_size=2000, strict=True)\n",
    "    ds = BatchData(ds, batch)\n",
    "    ds = PrefetchDataZMQ(ds, 1)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataFlow(DataFlow):\n",
    "  def __iter__(self, dir):\n",
    "    # load data from somewhere with Python, and yield them\n",
    "    video_set = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_set.append(os.path.splitext(file)[0])\n",
    "                \n",
    "      yield [digit, label]\n",
    "    \n",
    "class ProcessingDataFlow(DataFlow):\n",
    "  def __init__(self, ds):\n",
    "    self.ds = ds\n",
    "    \n",
    "  def reset_state(self):\n",
    "    self.ds.reset_state()\n",
    "\n",
    "  def __iter__(self):\n",
    "    for datapoint in self.ds:\n",
    "      # do something\n",
    "      yield new_datapoint\n",
    "# df = MyDataFlow()\n",
    "# df.reset_state()\n",
    "# for datapoint in df:\n",
    "#     print(datapoint[0], datapoint[1]\n",
    "df = MyDataFlow(dir='/my/data', shuffle=True)\n",
    "# resize the image component of each datapoint\n",
    "df = AugmentImageComponent(df, [imgaug.Resize((225, 225))])\n",
    "# group data into batches of size 128\n",
    "df = BatchData(df, 128)\n",
    "# start 3 processes to run the dataflow in parallel\n",
    "df = PrefetchDataZMQ(df, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
