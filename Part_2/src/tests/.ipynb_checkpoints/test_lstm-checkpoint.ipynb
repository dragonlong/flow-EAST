{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lxiaol9/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "=======================================================================================\n",
    "# This code runs with model.py, and process Video_16_3_2.mp4 with GT+Pred bounding box;\n",
    "# Originally adapted in March, modified in Sep.\n",
    "- Draw boxes\n",
    "- Model deplyment\n",
    "- Video Generation\n",
    "LOG: Sep. 26th we're testing the baseline with EAST model, test it on ICDAR 2013\n",
    "     Oct. 6th run script on remote cluster \n",
    "=======================================================================================\n",
    "\"\"\"\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import uuid\n",
    "import json\n",
    "import functools\n",
    "import logging\n",
    "import collections\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "# my own modules\n",
    "import _init_paths\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from utils.icdar import restore_rectangle\n",
    "import lanms\n",
    "from utils.eval import resize_image, sort_poly, detect\n",
    "from utils.icdar import load_annotations_solo, check_and_validate_polys\n",
    "from utils.nms_highlevel import intersection\n",
    "from lstm.model_rnn_east1 import ArrayModel\n",
    "from config.configrnn import get_config\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "############ Macros ############\n",
    "BASIC = \"basic\"\n",
    "CUDNN = \"cudnn\"\n",
    "BLOCK = \"block\"\n",
    "CONV  = \"conv2d\"\n",
    "CUDNN_INPUT_LINEAR_MODE = \"linear_input\"\n",
    "CUDNN_RNN_BIDIRECTION   = \"bidirection\"\n",
    "CUDNN_RNN_UNIDIRECTION = \"unidirection\"\n",
    "#\n",
    "flags = tf.flags\n",
    "flags.DEFINE_string(\"system\", \"local\", \"deciding running env\")\n",
    "flags.DEFINE_boolean('restore', True, 'whether to resotre from checkpoint')\n",
    "flags.DEFINE_boolean('partially_restore', False, 'whether to restore the weights of back-bone')\n",
    "flags.DEFINE_string(\"model\", \"test\", \"A type of model. Possible options are: small, medium, large.\")\n",
    "flags.DEFINE_integer(\"num_gpus\", 1, \"Larger than 1 will create multiple training replicas\")\n",
    "flags.DEFINE_string(\"rnn_mode\", CONV, \"one of CUDNN: BASIC, BLOCK\")\n",
    "flags.DEFINE_boolean('vis', False, \"whether we would use plt\")\n",
    "flags.DEFINE_integer('running_count', 0, \"know how many times we have run the model\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.0\n"
     ]
    }
   ],
   "source": [
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# used for iou searching and selecting TP, FP, FN #\n",
    "def eval_single_frame(target, box):\n",
    "    \"\"\"\n",
    "    input params:\n",
    "        target, python ordered dict\n",
    "        box, sorted boxes dict from predictions\n",
    "    \"\"\"\n",
    "    TP   = 0\n",
    "    FP   = 0\n",
    "    FN   = 0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    F_measure = 0\n",
    "    if not len(box['text_lines']) == 0:\n",
    "        for t in target:\n",
    "            d = np.array(t, dtype='int32')\n",
    "            is_best = 0\n",
    "            for m in box['text_lines']:\n",
    "                n = np.array([m['x0'], m['y0'], m['x1'], m['y1'], m['x2'],\n",
    "                              m['y2'], m['x3'], m['y3']], dtype='int32')\n",
    "\n",
    "                # pick out the best match\n",
    "                iou = intersection(n, d)\n",
    "                if iou>is_best:\n",
    "                    is_best = iou\n",
    "            if is_best > 0.5:\n",
    "                TP = TP+1\n",
    "            elif is_best == 0:\n",
    "                FN = FN +1\n",
    "            else:\n",
    "                FP = FP+1\n",
    "        if TP > 0:\n",
    "            precision = TP/(TP+FP)\n",
    "            recall    = TP/(TP+FN)\n",
    "            F_measure = 2*precision*recall/(precision+recall)\n",
    "    return precision, recall, F_measure\n",
    "\n",
    "\n",
    "def draw_illu(illu, rst):\n",
    "    for t in rst['text_lines']:\n",
    "        d = np.array([t['x0'], t['y0'], t['x1'], t['y1'], t['x2'],\n",
    "                      t['y2'], t['x3'], t['y3']], dtype='int32')\n",
    "        d = d.reshape(-1, 2)\n",
    "        cv2.polylines(illu, [d], isClosed=True, thickness=2, color=(0, 255, 0))\n",
    "    return illu\n",
    "\n",
    "\n",
    "def draw_illu_gt(illu, rst, p, r, f):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontScale = 0.3\n",
    "    fontColor = (255, 255, 255)\n",
    "    lineType = 1\n",
    "    infos = 'Precision ' + str(p)+ ', recall ' + str(r) + ', F_measure ' + str(f)\n",
    "    cv2.putText(illu, infos,\n",
    "                (2, 20),\n",
    "                font,\n",
    "                0.5,\n",
    "                (255, 0, 0),\n",
    "                lineType)\n",
    "    for t in rst:\n",
    "        d1 = t.reshape(-1, 2).astype(np.int32)\n",
    "        cv2.polylines(illu, [d1], isClosed=True, thickness=2, color=(0, 0, 0))\n",
    "        # bottomLeftCornerOfText = (int(t['x0']), int(t['y0']))\n",
    "\n",
    "    return illu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "resnet_v1_50/block1 (?, ?, ?, 256)\n",
      "resnet_v1_50/block2 (?, ?, ?, 512)\n",
      "resnet_v1_50/block3 (?, ?, ?, 1024)\n",
      "resnet_v1_50/block4 (?, ?, ?, 2048)\n",
      "Shape of f_0 (?, ?, ?, 2048)\n",
      "Shape of f_1 (?, ?, ?, 512)\n",
      "Shape of f_2 (?, ?, ?, 256)\n",
      "Shape of f_3 (?, ?, ?, 64)\n",
      "Shape of h_0 (?, ?, ?, 2048), g_0 (?, ?, ?, 2048)\n",
      "Shape of h_1 (?, ?, ?, 128), g_1 (?, ?, ?, 128)\n",
      "Shape of h_2 (?, ?, ?, 64), g_2 (?, ?, ?, 64)\n",
      "Shape of h_3 (?, ?, ?, 32), g_3 (?, ?, ?, 32)\n",
      "INFO:tensorflow:Restoring parameters from /work/cascades/lxiaol9/ARC/EAST/checkpoints/LSTM_east/20181005-192003/-26003\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "/io/opencv/modules/videoio/src/cap_ffmpeg.cpp:274: error: (-215) image->depth == 8 in function writeFrame\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-10026c823b0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-10026c823b0e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mnew_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_illu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m#                 new_img1 = draw_illu_gt(new_img.copy(), targets, precision, recall, f1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;31m# using for pre-testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /io/opencv/modules/videoio/src/cap_ffmpeg.cpp:274: error: (-215) image->depth == 8 in function writeFrame\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "def main():\n",
    "    checkpoint_path = '/work/cascades/lxiaol9/ARC/EAST/checkpoints/LSTM_east/'\n",
    "    idname1 = '20181005-192003'\n",
    "    idname2 = '-26003'\n",
    "    test_data_path = '/work/cascades/lxiaol9/ARC/EAST/data/ICDAR2015/test/'\n",
    "    save_path = '/work/cascades/lxiaol9/ARC/EAST/data/ICDAR2015/test_results_lstm/'\n",
    "    filename = '/work/cascades/lxiaol9/ARC/EAST/data/ICDAR2015/test/Video_6_3_2.mp4'\n",
    "    idx = 0  # initial frame number\n",
    "    config = get_config(FLAGS)\n",
    "    config.batch_size = 1\n",
    "    config.num_layers = 3\n",
    "    config.num_steps  = 10\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>Sort test video>>>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "    video_set = []\n",
    "    for root, dirs, files in os.walk(test_data_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_set.append(os.path.splitext(file)[0])\n",
    "    index = range(2, 6)\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise RuntimeError(\n",
    "            'Checkpoint `{}` not found'.format(checkpoint_path))\n",
    "\n",
    "    logger.info('loading model')\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>> Loading Model >>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "    gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "    input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images')\n",
    "    if FLAGS.running_count == 0:\n",
    "        reuse_variables=False\n",
    "    else:\n",
    "        reuse_variables=True\n",
    "    with tf.name_scope(\"Val\"):\n",
    "        # use placeholder to stand for input and targets\n",
    "        initializer = tf.random_normal_initializer()\n",
    "        x_val = tf.placeholder(tf.float32, shape=[None, config.num_steps, None, None, 3])\n",
    "        model = ArrayModel(False, config, x_val, reuse_variables=reuse_variables, initializer=initializer)\n",
    "        FLAGS.running_count+=1\n",
    "    var_total = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>>> restore the model from weights>>>>>>>>#\n",
    "    soft_placement = False\n",
    "    saver = tf.train.Saver()\n",
    "    config_proto = tf.ConfigProto(allow_soft_placement = soft_placement)\n",
    "    # with sv.managed_session(config=config_proto) as session:\n",
    "    #     if FLAGS.restore:\n",
    "    #         print('continue training from previous checkpoint')\n",
    "    #         # ckpt = tf.train.latest_checkpoint(FLAGS.checkpoints_path)\n",
    "    #         ckpt = checkpoint_path + idname1 + '/' + idname2\n",
    "    #         sv.saver.restore(session, ckpt)\n",
    "    model_path = checkpoint_path + idname1 + '/' + idname2\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "    logger.info('Restore from {}'.format(model_path))\n",
    "    saver.restore(sess, model_path)\n",
    "    #>>>>>>>>>>>>>>>>>>>>>>Start evaluation>>>>>>>>>>>>>>>>>>>>>>>>>#\n",
    "    P_test = []\n",
    "    R_test = []\n",
    "    f1_test = []\n",
    "    for k in index:\n",
    "        P_video = []\n",
    "        R_video = []\n",
    "        f1_video = []\n",
    "        video_save = save_path + video_set[k] + idname1 + '_' + idname2 + '.avi'\n",
    "        t_start = time.time()\n",
    "        # sort up all the paths\n",
    "        xml_solo_path = test_data_path + video_set[k]\n",
    "        raw_video_path = test_data_path + video_set[k]+'.mp4'\n",
    "        cap = cv2.VideoCapture(raw_video_path)\n",
    "        frame_width = int(cap.get(3))\n",
    "        frame_height = int(cap.get(4))\n",
    "        cnt_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        out = cv2.VideoWriter(video_save, cv2.VideoWriter_fourcc('M','J','P','G'), 10, (frame_width,frame_height))\n",
    "        # 1. load both polys and tags; 2. generate geo maps(the format of polys and tags need to match)\n",
    "#         polys_array_list, tags_array_list, id_list_list, frame_num = load_annotations_solo(xml_solo_path, \\\n",
    "#                     1, cnt_frame, frame_width, frame_height)\n",
    "        #>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>loop over frames in the time steps >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "        for i in range(int(cnt_frame/config.num_steps)):\n",
    "            data_seq = np.zeros((1, config.num_steps, 512, 512, 3), dtype=np.float32)\n",
    "            data_original = np.zeros((1, config.num_steps, frame_height, frame_width, 3), dtype=np.float32)\n",
    "            for k in range(config.num_steps):\n",
    "                ret, frame = cap.read()\n",
    "                im_resized = cv2.resize(frame, (int(512), int(512)))\n",
    "#                 im_resized = frame[0:512, 0:512, :]\n",
    "                data_original[0, k, :, :,:] = frame\n",
    "                data_seq[0, k, :, : , :] = im_resized\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>>Now it's time to run the model>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            state = sess.run(model.initial_state)\n",
    "            # tensors dict to run\n",
    "            fetches = {\n",
    "                    \"score_map\": model.score_map_set,\n",
    "                    \"geometry_map\": model.geometry_set\n",
    "                    }\n",
    "            feed_dict = {}\n",
    "            feed_dict[model.input_data] = data_seq\n",
    "            for i, (c, h) in enumerate(model.initial_state):\n",
    "                feed_dict[c] = state[i].c\n",
    "                feed_dict[h] = state[i].h\n",
    "            timer = collections.OrderedDict([\n",
    "                ('net', 0),\n",
    "                ('restore', 0),\n",
    "                ('nms', 0)\n",
    "            ])\n",
    "            start = time.time()\n",
    "            vals = sess.run(fetches, feed_dict=feed_dict)\n",
    "            timer['net'] = time.time() - start\n",
    "            #>>>>>>>>>>>>>>>>>>>>>>>>Okay!!!We could evalute the results now>>>>>>>>>>>>>>>>>>>\n",
    "            for j in range(config.num_steps):\n",
    "                rtparams = collections.OrderedDict()\n",
    "                rtparams['start_time'] = datetime.datetime.now().isoformat()\n",
    "                rtparams['image_size'] = '{}x{}'.format(frame_width, frame_height)\n",
    "                # im_resized, (ratio_h, ratio_w) = resize_image(img)\n",
    "                ratio_h, ratio_w = 512/frame_height, 512/frame_width\n",
    "                rtparams['working_size'] = '{}x{}'.format(512, 512)\n",
    "                # results refinement via NMS\n",
    "                score = vals[\"score_map\"][j]\n",
    "                geometry = vals[\"geometry_map\"][j]\n",
    "                boxes, timer = detect(score_map=score, geo_map=geometry, timer=timer)\n",
    "                logger.info('net {:.0f}ms, restore {:.0f}ms, nms {:.0f}ms'.format(\n",
    "                    timer['net']*1000, timer['restore']*1000, timer['nms']*1000))\n",
    "                if boxes is not None:\n",
    "                    scores = boxes[:,8].reshape(-1)\n",
    "                    boxes = boxes[:, :8].reshape((-1, 4, 2))\n",
    "                    boxes[:, :, 0] /= ratio_w\n",
    "                    boxes[:, :, 1] /= ratio_h\n",
    "\n",
    "                duration = time.time() - start\n",
    "                timer['overall'] = duration\n",
    "                logger.info('[timing] {}'.format(duration))\n",
    "                text_lines = []\n",
    "                if boxes is not None:\n",
    "                    text_lines = []\n",
    "                    for box, score in zip(boxes, scores):\n",
    "                        box = sort_poly(box.astype(np.int32))\n",
    "                        if np.linalg.norm(box[0] - box[1]) < 5 or np.linalg.norm(box[3]-box[0]) < 5:\n",
    "                            continue\n",
    "                        tl = collections.OrderedDict(zip(\n",
    "                            ['x0', 'y0', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3'],\n",
    "                            map(float, box.flatten())))\n",
    "                        tl['score'] = float(score)\n",
    "                        text_lines.append(tl)\n",
    "                pred = {\n",
    "                    'text_lines': text_lines,\n",
    "                    'rtparams': rtparams,\n",
    "                    'timing': timer,\n",
    "                }\n",
    "#                 text_polys, text_tags = polys_array_list[i*10+j], tags_array_list[i*10+j]\n",
    "#                 text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (frame_height, frame_width))\n",
    "                # out.write(new_img)\n",
    "                #>>>>>>>>>>>>>>>>>>>>>>>>Evaluation>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "#                 targets = text_polys\n",
    "#                 precision, recall, f1 = eval_single_frame(targets, pred)\n",
    "                precision, recall, f1 = 0, 0, 0\n",
    "                P_video.append(precision)\n",
    "                R_video.append(recall)\n",
    "                f1_video.append(f1)\n",
    "                img = data_original[0, j, :, :,:]\n",
    "                new_img = draw_illu(img.copy(), pred)\n",
    "#                 new_img1 = draw_illu_gt(new_img.copy(), targets, precision, recall, f1)\n",
    "                out.write(new_img)\n",
    "                # using for pre-testing\n",
    "                FLAGS.vis = True \n",
    "                if j == 0 and FLAGS.vis:\n",
    "                    fig1 = plt.figure(figsize=(20, 10))\n",
    "                    fig1.add_subplot(1, 2, 1)\n",
    "                    plt.imshow(new_img )\n",
    "                    plt.title(\"Text Detection with fine-tuned EAST\")\n",
    "                    fig1.add_subplot(1, 2, 2)\n",
    "                    plt.imshow(new_img)\n",
    "                    plt.title('Text Detection Results Comparison')\n",
    "                    import pdb; pdb.set_trace()\n",
    "                    plt.show()\n",
    "                    \n",
    "                if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "            # evaluation on ret and gt\n",
    "        P_test.append(np.array(P_video, dtype=np.float32))\n",
    "        R_test.append(np.array(R_video, dtype=np.float32))\n",
    "        f1_test.append(np.array(f1_video, dtype=np.float32))\n",
    "        print(P_video)\n",
    "        print(R_video)\n",
    "        print(f1_video)\n",
    "        print(\"testing results are P:{}, R:{}, F1:{} on \".format(sum(P_video)/cnt_frame, sum(R_video)/cnt_frame, sum(f1_video)/cnt_frame)+video_set[k])\n",
    "        cap.release()\n",
    "        out.release()                # results refinement via NMS\n",
    "    print('here is the precision')\n",
    "    for item in P_test:\n",
    "        print(np.mean(item))\n",
    "    print('here is the recall')\n",
    "    for item in R_test:\n",
    "        print(np.mean(item))\n",
    "    print('here is the f-score')\n",
    "    for item in f1_test:\n",
    "        print(np.mean(item))\n",
    "    print(video_set)\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
